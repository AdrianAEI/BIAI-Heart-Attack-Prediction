import warnings
warnings.filterwarnings('ignore')

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px


from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler, OneHotEncoder
from sklearn.cluster import KMeans
from sklearn.manifold import TSNE

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score, confusion_matrix
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import itertools

# Load the dataset
df = pd.read_csv("kaggle/input/heart.csv")
df.head()

# Check the number of distinct values in each column
for column in df.columns:
    num_distinct_values = len(df[column].unique())
    print(f"{column}: {num_distinct_values} distinct values")
	
# Remove duplicate rows from the dataset
df = df.drop_duplicates()

# Map values in 'thall' column from 0 to 2 to 2 to 3
df['thall'] = df['thall'].replace(0, 2)

# Map values in 'cp' column to their respective categories
cp_mapping = {0: 'typical angina',
              1: 'atypical angina',
              2: 'non-anginal pain',
              3: 'asymptomatic'}
df['cp'] = df['cp'].map(cp_mapping)

# Map values in 'slp' column to their respective categories
slp_mapping = {0: 'unsloping',
               1: 'flat',
               2: 'downsloping'}
df['slp'] = df['slp'].map(slp_mapping)

# Map values in 'thall' column to their respective categories
thall_mapping = {1: 'fixed defect',
                 2: 'normal',
                 3: 'reversible defect'}
df['thall'] = df['thall'].map(thall_mapping)

# Map values in 'restecg' column to their respective categories
rest_ecg_mapping = {0: 'normal',
                    1: 'ST-T wave abnormality',
                    2: 'left ventricular hypertrophy'}
df['restecg'] = df['restecg'].map(rest_ecg_mapping)

# Map values in 'sex' column to 'male' and 'female'
sex_mapping = {1: 'male',
               0: 'female'}
df['sex'] = df['sex'].map(sex_mapping)

# Visualize the distribution of the 'age' column using a histogram
plt.hist(df['age'], bins=10, edgecolor='black')
plt.title('Age Distribution of Patients')
plt.xlabel('Age')
plt.ylabel('Frequency')
plt.show()

# Create a copy of the dataset for further processing
data = df.copy()

# Define categorical and numerical columns
categorical_columns = ['sex', 'cp', 'restecg', 'slp', 'thall']
numerical_columns = ['age', 'trtbps', 'chol', 'fbs', 'thalachh', 'exng', 'oldpeak', 'caa', 'output']

# Create dummy variables for categorical columns
dummy_variables = pd.get_dummies(data, columns=categorical_columns, drop_first=False)

# Initialize a StandardScaler
scaler = StandardScaler()

# Scale the numerical columns
scaled_numerical = scaler.fit_transform(data[numerical_columns])

# Convert the scaled numerical columns to a DataFrame
scaled_numerical_df = pd.DataFrame(scaled_numerical, columns=numerical_columns)

# Drop the original numerical columns from the dummy variables DataFrame
dummy_variables = dummy_variables.drop(numerical_columns, axis=1)

# Concatenate the dummy variables and scaled numerical columns
processed_df = pd.concat([dummy_variables, scaled_numerical_df], axis=1)

# Calculate the correlation matrix of the processed DataFrame
correlation_matrix = processed_df.corr()

# Plot a heatmap of the correlation matrix
plt.figure(figsize=(15, 10))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5, fmt='.2f')
plt.title("Correlation Matrix Heatmap")
plt.show()

# Calculate the correlation of each feature with the target column ('output')
corr = processed_df.corr()
target_corr = corr['output'].drop('output')

# Sort the correlation values in descending order
target_corr_sorted = target_corr.sort_values(ascending=False)

# Plot a heatmap of the correlations with the target column
sns.set(font_scale=0.8)
sns.set_style("white")
sns.set_palette("PuBuGn_d")
sns.heatmap(target_corr_sorted.to_frame(), cmap="coolwarm", annot=True, fmt='.2f')
plt.title('Correlation with Heart attack')
plt.show()

# First, handle the categorical columns
le = LabelEncoder()

# Encode categorical columns using LabelEncoder
for col in ['sex', 'cp', 'restecg', 'slp', 'thall']:
    df[col] = le.fit_transform(df[col])

# Scale numerical features using StandardScaler
scaler = StandardScaler()

for col in ['age', 'trtbps', 'chol', 'fbs', 'thalachh', 'exng', 'oldpeak', 'caa']:
    df[col] = scaler.fit_transform(df[col].values.reshape(-1, 1))

# Compute t-SNE
X = df.drop('output', axis=1).values  # dropping the target column
y = df['output'].values  # the target column

tsne = TSNE(n_components=2, random_state=42)
X_tsne = tsne.fit_transform(X)

# Create a new DataFrame for the two-dimensional t-SNE representation
df_tsne = pd.DataFrame(data=X_tsne, columns=['Component 1', 'Component 2'])
df_tsne['Target'] = y

# Visualize t-SNE using Plotly
fig = px.scatter(df_tsne, x='Component 1', y='Component 2', color='Target',
                 title='2 Component t-SNE', template='plotly')
fig.show()

categorical_columns = ['sex', 'cp', 'restecg', 'slp', 'thall']
numerical_columns = ['age', 'trtbps', 'chol', 'fbs', 'thalachh', 'exng', 'oldpeak', 'caa']

# Split data into features (X) and target (y)
X = data.drop('output', axis=1)
y = data['output']

# Split data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define the preprocessor using ColumnTransformer
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numerical_columns),
        ('cat', OneHotEncoder(), categorical_columns)])

# Fit and transform the training data using the preprocessor
X_train = preprocessor.fit_transform(X_train)
X_test = preprocessor.transform(X_test)

# Define a function to evaluate model performance
def evaluate_model(model, X_train, y_train, X_test, y_test):
    model.fit(X_train, y_train)
    train_preds = model.predict(X_train)
    test_preds = model.predict(X_test)
    train_acc = accuracy_score(y_train, train_preds)
    test_acc = accuracy_score(y_test, test_preds)
    return train_acc, test_acc, test_preds

# Define a function to plot a confusion matrix
def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):
    # Plot the confusion matrix
    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)

    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]

    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, cm[i, j], horizontalalignment="center", color="white" if cm[i, j] > thresh else "black")

    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')

models = [
    LogisticRegression(),
    SVC(),
    DecisionTreeClassifier(),
]

model_names = ["Logistic Regression", "SVC", "Decision Trees"]

train_results = []
test_results = []

# Evaluate and compare the performance of each model
for model in models:
    train_acc, test_acc, y_pred = evaluate_model(model, X_train, y_train, X_test, y_test)
    train_results.append(train_acc)
    test_results.append(test_acc)
    cm = confusion_matrix(y_test, y_pred)
    plot_confusion_matrix(cm, classes=['No Disease', 'Disease'], title=model.__class__.__name__)
    plt.show()

# Plot the accuracy of each model
plt.figure(figsize=(10, 5))
plt.plot(model_names, train_results, 'o-', label="Training Accuracy")
plt.plot(model_names, test_results, 'o-', label="Testing Accuracy")
plt.ylabel("Accuracy")
plt.xticks(rotation=45)
plt.legend()
plt.title("Model Comparisons - Accuracy")
plt.show()

# Logistic Regression with cross-validation
lr = LogisticRegressionCV(cv=5)
lr.fit(X_train, y_train)
y_pred_lr = lr.predict(X_test)

# SVM with grid search for hyperparameter tuning
from sklearn.model_selection import GridSearchCV
from sklearn.svm import SVC
param_grid = {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf']}
svm = GridSearchCV(SVC(), param_grid, cv=5)
svm.fit(X_train, y_train)
y_pred_svm = svm.predict(X_test)

# Decision Tree with randomized search for hyperparameter tuning
from sklearn.model_selection import RandomizedSearchCV
from sklearn.tree import DecisionTreeClassifier
param_dist = {'max_depth': [3, None], 'min_samples_split': [2, 3, 4]}
dtc = RandomizedSearchCV(DecisionTreeClassifier(), param_dist, cv=5)
dtc.fit(X_train, y_train)
y_pred_dtc = dtc.predict(X_test)

# Evaluate and compare the performance of each model
for model in models:
    train_acc, test_acc, y_pred = evaluate_model(model, X_train, y_train, X_test, y_test)
    train_results.append(train_acc)
    test_results.append(test_acc)
    cm = confusion_matrix(y_test, y_pred)
    plot_confusion_matrix(cm, classes=['No Disease', 'Disease'], title=model.__class__.__name__)
    plt.show()

# Evaluate and print the performance of Logistic Regression CV
train_acc_lr, test_acc_lr, y_pred_lr = evaluate_model(lr, X_train, y_train, X_test, y_test)
print("Logistic Regression CV:")
print("Train Accuracy:", train_acc_lr)
print("Test Accuracy:", test_acc_lr)

# Evaluate and print the performance of SVM with Grid Search CV
train_acc_svm, test_acc_svm, y_pred_svm = evaluate_model(svm, X_train, y_train, X_test, y_test)
print("SVM (Support Vector Machine) with Grid Search CV:")
print("Train Accuracy:", train_acc_svm)
print("Test Accuracy:", test_acc_svm)

# Evaluate and print the performance of Decision Tree with Randomized Search CV
train_acc_dtc, test_acc_dtc, y_pred_dtc = evaluate_model(dtc, X_train, y_train, X_test, y_test)
print("Decision Tree with Randomized Search CV:")
print("Train Accuracy:", train_acc_dtc)
print("Test Accuracy:", test_acc_dtc)

# Plot the accuracy of each model
plt.figure(figsize=(10, 5))
model_names = ["Logistic Regression CV", "SVM with Grid Search CV", "Decision Tree with Randomized Search CV"]
train_results = [train_acc_lr, train_acc_svm, train_acc_dtc]
test_results = [test_acc_lr, test_acc_svm, test_acc_dtc]
plt.plot(model_names, train_results, 'o-', label="Training Accuracy")
plt.plot(model_names, test_results, 'o-', label="Testing Accuracy")
plt.ylabel("Accuracy")
plt.xticks(rotation=45)
plt.legend()
plt.title("Model Comparisons - Accuracy")
plt.show()

# Score of each model on the test set
for model in models:
    model.score(X_test, y_test)

# Building a neural network model with TensorFlow
from tensorflow import keras

model = keras.Sequential(
    [
        keras.layers.Dense(
            256, activation="relu", input_shape=[23]
        ),
        keras.layers.Dense(515, activation="relu"),
        keras.layers.Dropout(0.3),
        keras.layers.Dense(50, activation="relu"),
        keras.layers.Dropout(0.3),
        keras.layers.Dense(1, activation="sigmoid"),
    ]
)
model.summary()

model.compile(optimizer="Adam", loss="binary_crossentropy", metrics=["binary_accuracy"])

early_stopping = keras.callbacks.EarlyStopping(
    patience=20, min_delta=0.001, restore_best_weights=True
)
history = model.fit(
    X_train,
    y_train,
    validation_data=(X_test, y_test),
    batch_size=15,
    epochs=50,
    callbacks=[early_stopping],
    verbose=1,
)

model.evaluate(X_test, y_test)

# Print classification report
print(classification_report(y_test, predictions))

# Plot confusion matrix for the neural network model
cm = confusion_matrix(y_test, predictions)
plot_confusion_matrix(cm, classes=['No Disease', 'Disease'], title=model.__class__.__name__)
plt.show()
